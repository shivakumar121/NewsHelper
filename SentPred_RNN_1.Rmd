---
title: "Predict Sentiment of Top News Stories From Popular Media's Articles"
output:
  md_document:
    variant: markdown_github
---
# Overview of the method

The ultimate goal is to be able to classify top news articles from popular media into positive or negative sentiment. We all can appreciate that more and more people today are relying on digital media to obtain their daily news. Consequently, there is a lot of competition among news outlets to attract readership. While competition is great, the unfortunate down-side in this case is that the news articles are becoming increasingly more polarized. Polarized articles tend to excite more emotion, and therefore, more likes (or dislikes) which in-turn leads to more attention.

The hypothesis in this case is that we should be able to obtain a pattern of the underlying propensities of each news media towards certain issues. For example, certain left-leaning or right-leaning news outlet might cover the same issue in completely contrasting light. This document shows preliminary analysis to show that it is possible to extract such patterns from top news headlines. 

The scripts here will download on-the-fly the currently top news in popular media, and classify them into groups based on their conveyed sentiment.

## Learn From Existing Data

The first step is to learn how to predict sentiment using existing data. In this example we'll use IMDB movie review database to obtain a list of most polar words. 
As this is a proof of concept, we are not going to train an NLP model, but show that words do have the power to perform this task.

### Read data and count the frequency of words in positive Vs negative reviews.

We will create two data-frames of positive and negative reviews. This would form our vocabulary for this task.

```{r}
library(stringr)
TrainDataDir <- "/media/TDI/TDIChallenge/Question_3/News/IMDBReviews/aclImdb/train"
File_Names <- list.files(path = paste0(TrainDataDir,"/pos"), pattern = ".txt$")
#MaxFiles <- 200
MaxFiles <- length(File_Names)
Pos.reviews.df <- as.data.frame(matrix(nrow = MaxFiles, ncol = 2))
colnames(Pos.reviews.df) <- c("Nome_of_File", "ReviewContent")
FileName_temp <- NULL
MaxWords <- 200
for (i in 1:MaxFiles) ### change to (length(File_Names)) later
{
  Pos.reviews.df$Nome_of_File[i] <- File_Names[i]
  CurrentFileName <- paste0(TrainDataDir,"/pos/", File_Names[i])
  TempReview <- readChar(CurrentFileName, nchars = 2000)
  SpaceLocations <- str_locate_all(TempReview, pattern = " ")
  if (length(SpaceLocations[[1]][,1]) > MaxWords)
  {
    TempReview <- str_sub(TempReview, start =1, end = SpaceLocations[[1]][MaxWords,1])
  }
  Pos.reviews.df$ReviewContent[i] <- TempReview
}

### Now the same for negative reviews ###

File_Names <- list.files(path = paste0(TrainDataDir,"/neg"), pattern = ".txt$")
MaxFiles <- length(File_Names)
Neg.reviews.df <- as.data.frame(matrix(nrow = MaxFiles, ncol = 2))
colnames(Neg.reviews.df) <- c("Nome_of_File", "ReviewContent")
FileName_temp <- NULL

for (i in 1:MaxFiles) ### change to (length(File_Names)) later
{
  Neg.reviews.df$Nome_of_File[i] <- File_Names[i]
  CurrentFileName <- paste0(TrainDataDir,"/neg/", File_Names[i])
  TempReview <- readChar(CurrentFileName, nchars = 2000)
  SpaceLocations <- str_locate_all(TempReview, pattern = " ")
  if (length(SpaceLocations[[1]][,1]) > MaxWords)
  {
    TempReview <- str_sub(TempReview, start =1, end = SpaceLocations[[1]][MaxWords,1])
  }
  Neg.reviews.df$ReviewContent[i] <- TempReview
}
```
## Vectorize The Words

```{r}
library (stringr)
VocabList <- c(str_split (Pos.reviews.df$ReviewContent, pattern = " "), str_split (Neg.reviews.df$ReviewContent, pattern = " "))
VocabList <- unlist(VocabList)
VocabList <- gsub("[^[:alnum:] ]", "", VocabList)
VocabList <- tolower(VocabList)
VocabList <- VocabList[which(nchar(VocabList) > 0)]
VocabListUnique <- unique(VocabList)
VocabFreq <- table(VocabList)
VocabListEndoded.df <- data.frame(matrix (nrow = length(VocabListUnique), ncol = 2))
colnames(VocabListEndoded.df) <- c("Word", "Int")
VocabListEndoded.df$Word <- VocabListUnique
VocabListEndoded.df$Int <- 1:length(VocabListUnique)
VocabListEndoded <- as.list(1:length(VocabListUnique))
names(VocabListEndoded) <- VocabListUnique
WordToInt <- NULL
## Add Word to Integer column to Positive reviews dataset ####
for (i in 1:nrow(Pos.reviews.df))
{
  TempWordToInt <- rep(0, times = MaxWords)
  Review_Words <- str_split(Pos.reviews.df$ReviewContent[i], pattern = " ")
  Review_Words <- unlist(Review_Words)
  Review_Words <- gsub("[^[:alnum:] ]", "", Review_Words)
  Review_Words <- tolower(Review_Words)
  Review_Words <- Review_Words[nchar(Review_Words) > 0]
  if (length(Review_Words) > MaxWords)
  {
    Review_Words <- Review_Words[1:MaxWords]
  }
  
  TempIndex <- match(Review_Words, VocabListEndoded.df$Word)
  WordsListLength <- length(Review_Words)
  TempWordToInt[(MaxWords-WordsListLength+1):MaxWords] <- VocabListEndoded.df$Int[TempIndex]
  WordToInt <- c(WordToInt,paste0(TempWordToInt, collapse = ","))
}
Pos.reviews.df$WordToInt <- WordToInt
## Add Word to Integer column to Negative reviews dataset in the same manner as above ####
WordToInt <- NULL
for (i in 1:nrow(Neg.reviews.df))
{
  TempWordToInt <- rep(0, times = MaxWords)
  Review_Words <- str_split(Neg.reviews.df$ReviewContent[i], pattern = " ")
  Review_Words <- unlist(Review_Words)
  Review_Words <- gsub("[^[:alnum:] ]", "", Review_Words)
  Review_Words <- tolower(Review_Words)
  Review_Words <- Review_Words[nchar(Review_Words) > 0]
  if (length(Review_Words) > MaxWords)
  {
    Review_Words <- Review_Words[1:MaxWords]
  }
  
  TempIndex <- match(Review_Words, VocabListEndoded.df$Word)
  WordsListLength <- length(Review_Words)
  TempWordToInt[(MaxWords-WordsListLength+1):MaxWords] <- VocabListEndoded.df$Int[TempIndex]
  WordToInt <- c(WordToInt,paste0(TempWordToInt, collapse = ","))
}
Neg.reviews.df$WordToInt <- WordToInt
```

## Build the Graphs

```{r}
## Split inputs into train and test and add output labels ####
OutPutLabel <- rep(1, times = nrow(Pos.reviews.df))
Pos.reviews.df$OutputLabel <- OutPutLabel
OutPutLabel <- rep(0, times = nrow(Neg.reviews.df))
Neg.reviews.df$OutputLabel <- OutPutLabel
Merged.reviews.df <- as.data.frame (rbind(Pos.reviews.df, Neg.reviews.df))
## Randomize the data frame
Index <- sample (1:nrow(Merged.reviews.df), size = nrow(Merged.reviews.df), replace = F)
Merged.reviews.df <- Merged.reviews.df[Index,]
SplitFrac <- 0.2
TestIndex <- sample(1:nrow(Merged.reviews.df), size = SplitFrac*nrow(Merged.reviews.df))
Merged.reviews.df.Test <- Merged.reviews.df[TestIndex,]
Merged.reviews.df.Train <- Merged.reviews.df[-TestIndex,]


library(keras)
VocabSize <- length(VocabListUnique) + 1 ### Add the padding of 0 
main_input <- layer_input(shape = c(MaxWords), dtype = 'int64', name = 'main_input')
EmbedSize <- 512
lstm_out <- main_input %>% 
  layer_embedding(input_dim = VocabSize, output_dim = EmbedSize, input_length = MaxWords) %>% 
  layer_lstm(units = 32)
auxiliary_output <- lstm_out %>% 
  layer_dense(units = 1, activation = 'sigmoid', name = 'aux_output')

#auxiliary_input <- layer_input(shape = c(5), name = 'aux_input')

main_output <- lstm_out %>%  
  layer_dense(units = 64, activation = 'relu') %>% 
  layer_dense(units = 1, activation = 'sigmoid', name = 'main_output')

model <- keras_model(
  inputs = main_input, 
  outputs = c(main_output, auxiliary_output)
)

summary(model)

model %>% compile(
  optimizer = 'rmsprop',
  loss = 'binary_crossentropy',
  loss_weights = c(1.0, 0.2)
)
InputGen <- str_split(Merged.reviews.df.Train$WordToInt, pattern = ",")
InputGenArrary <- NULL
InputGenArrary <- as.array(matrix(nrow=nrow(Merged.reviews.df.Train),ncol=MaxWords))
for (i in 1:length(InputGen))
{
  InputGenArrary[i,] <- as.integer(InputGen[[i]])
}
#tt=as.array(tt)
history <- model %>% fit(
  x = (main_input = InputGenArrary),
  y = list(main_output = Merged.reviews.df.Train$OutputLabel, aux_output = Merged.reviews.df.Train$OutputLabel),
  epochs = 2,
  batch_size = 32
)

plot(history)
#########################################################
LstmSize = 256
LstmLayers = 1
BatchSize = 500
LearningRate = 0.001

# Create the graph object
graph = tf.Graph()
# Add nodes to the graph
with graph.as_default():
    inputs_ = tf.placeholder(tf.int32, [None, None], name='inputs')
    labels_ = tf.placeholder(tf.int32, [None, None], name='labels')
    keep_prob = tf.placeholder(tf.float32, name='keep_prob')
```


## Extract Polarity Of The Words

We now need the create a data-frame with all words in our vocabulary and their associated polarity.

```{r}
### First convert all positive and negtive reviews into word levels, or vocabulary ##
library (stringr)
#VocabList <- NULL
#VocabList <- as.list (VocabList)
#for (i in 1:1000)
#{
#  VocabList <- c(VocabList, str_split(Pos.reviews.df$ReviewContent))
#}
VocabList <- c(str_split (Pos.reviews.df$ReviewContent, pattern = " "), str_split (Neg.reviews.df$ReviewContent, pattern = " "))
VocabList <- unlist(VocabList)
VocabList <- gsub("[^[:alnum:] ]", "", VocabList)
VocabList <- tolower(VocabList)
VocabList <- VocabList[which(nchar(VocabList) > 0)]
VocabListUnique <- unique(VocabList)
VocabFreq <- table(VocabList)
VocabListEndoded <- as.list(1:length(VocabListUnique))
names(VocabListEndoded) <- VocabListUnique
#hist(VocabFreq)
#### Encode positive and negative labels as 0 or 1 ######

VocabFreq_sorted <- sort(VocabFreq, decreasing = T)
##### Positive Vobulary ####
Pos_VocabList <- str_split (Pos.reviews.df$ReviewContent, pattern = " ")
Pos_VocabList <- unlist(Pos_VocabList)
Pos_VocabList <- gsub("[^[:alnum:] ]", "", Pos_VocabList)
Pos_VocabList <- tolower(Pos_VocabList)
Pos_VocabList_Freq <- table(Pos_VocabList)
#### Negative vocabulary ####
Neg_VocabList <- str_split (Neg.reviews.df$ReviewContent, pattern = " ")
Neg_VocabList <- unlist(Neg_VocabList)
Neg_VocabList <- gsub("[^[:alnum:] ]", "", Neg_VocabList)
Neg_VocabList <- tolower(Neg_VocabList)
Neg_VocabList_Freq <- table(Neg_VocabList)
#### Find pos neg differences ####
HighVocabFreq <- VocabFreq[which(VocabFreq > 200)]
WordsOfInterest <- names(HighVocabFreq)
WordDiffs.df <- as.data.frame (matrix(nrow = length(WordsOfInterest), ncol = 2))
colnames(WordDiffs.df) <- c("Word", "Freq_Diff")
for (i in 1:length(WordsOfInterest))
{
  Index <- which(names(Pos_VocabList_Freq) == WordsOfInterest[i])
  if (length(Index) == 0)
  {
    TempFreqPos <- 0
  } else
  {
    TempFreqPos <- Pos_VocabList_Freq[Index]
  }
  
  Index <- which(names(Neg_VocabList_Freq) == WordsOfInterest[i])
  if (length(Index) == 0)
  {
    TempFreqNeg <- 0
  } else
  {
    TempFreqNeg <- Neg_VocabList_Freq[Index]
  }
  TempFreqDiff <- (TempFreqPos+1) / (TempFreqNeg+1)
  WordDiffs.df$Word[i] <- WordsOfInterest[i]
  WordDiffs.df$Freq_Diff[i] <- TempFreqDiff
}
TempPosIndex <- match(sort(WordDiffs.df$Freq_Diff,decreasing = T)[1:20],WordDiffs.df$Freq_Diff)
TempNegIndex <- match(sort(WordDiffs.df$Freq_Diff, decreasing = F)[1:20], WordDiffs.df$Freq_Diff)
#WordDiffs.df[c(TempPosIndex,TempNegIndex),]
#hist(WordDiffs.df$Freq_Diff)
TempIndex <- match(sort(WordDiffs.df$Freq_Diff, decreasing = T)[1:200], WordDiffs.df$Freq_Diff)
#WordDiffs.df$Word[TempIndex]
#WordDiffs.df$Word[which(sort(WordDiffs.df$Freq_Diff, decreasing = F)[1:10]]
```
## Get Latest Top News Articles From Popular US Media

We'll now use an API from [News API](https://newsapi.org/) to obtain the top news at this time. After that, we'll process the news articles and make a data-frame that contains the number of times the most polar words occur in those articles.


```{r}
library(httr)
library(jsonlite)
tt_sources=GET("https://newsapi.org/v1/sources?language=en&apiKey=5f023192aa0c44bba4fe2fe1ceb9ca72")
tt2 <-rawToChar(tt_sources$content)
tt2 <- fromJSON(tt2)
MyIndex <- which( tt2$sources$country == "us")
tt2$sources$name[MyIndex]
MyMedia <- tt2$sources$name[MyIndex]
MyMediaIds <- tt2$sources$id[MyIndex]
AllGetArticles <- NULL
AllGetArticles <- as.list(AllGetArticles)
OkStatusIndex <- NULL
for (i in 1:length(MyMedia))
{
  #TempGet <- tt=GET("https://newsapi.org/v1/articles?source=the-next-web&sortBy=latest&apiKey=5f023192aa0c44bba4fe2fe1ceb9ca72")
  GetCommand <- paste0("https://newsapi.org/v1/articles?source=", MyMediaIds[i], "&sortBy=top&apiKey=5f023192aa0c44bba4fe2fe1ceb9ca72")
  TempGet <- GET(GetCommand)
  TempGet <-rawToChar(TempGet$content)
  TempGet <- fromJSON(TempGet)
  if (TempGet$status == "ok")
  {
    OkStatusIndex <- c(OkStatusIndex, i)
  }
  AllGetArticles[[i]] <- TempGet
}
#AllGetArticles_ok <- AllGetArticles[[OkStatusIndex]]
### Make data frame of article qualities ####

HighDiff <- sort(WordDiffs.df$Freq_Diff, decreasing = T)[1:200]
HighIndex <- NULL
for (i in 1:length(HighDiff))
{
  HighIndex <- c(HighIndex, which(WordDiffs.df$Freq_Diff == HighDiff[i]))
}
LowIndex <- NULL
LowDiff <- sort(WordDiffs.df$Freq_Diff, decreasing = F)[1:200]
for (i in 1:length(HighDiff))
{
  LowIndex <- c(LowIndex, which(WordDiffs.df$Freq_Diff == LowDiff[i]))
}
PolarIndex <- sort(unique(c(HighIndex, LowIndex)))
ArticleTypes.df <- as.data.frame(matrix(nrow = length(OkStatusIndex), ncol = length(PolarIndex)))
PolarWords <- WordDiffs.df$Word[PolarIndex]
colnames(ArticleTypes.df) <- PolarWords
MyRowNames <- NULL
iii <- 1
for (CurrentIndex in OkStatusIndex)
{
  ArticleText <- c(AllGetArticles[[CurrentIndex]]$articles[2], AllGetArticles[[CurrentIndex]]$articles[3])
  MyRowNames <- c(MyRowNames, AllGetArticles[[CurrentIndex]]$source)
  ArticleText <- unlist(ArticleText)
  ArticleText <- str_split(ArticleText, pattern = " ")
  ArticleText <- unlist(ArticleText)
  ArticleText <- gsub("[^[:alnum:] ]", "", ArticleText)
  ArticleText <- tolower (ArticleText)
  ArticleTextFreq <- table(ArticleText)
  for (CurrentWord in PolarWords)
  {
    Index <- which(names(ArticleTextFreq) == CurrentWord)
    if (length(Index) > 0)
    {
      CurrentWordFreq <- ArticleTextFreq[Index]
    } else
    {
      CurrentWordFreq <- 0
    }
    ColIndex <- which(colnames(ArticleTypes.df) == CurrentWord)
    ArticleTypes.df[iii, ColIndex] <- CurrentWordFreq
  }
  iii <- iii + 1
}
rownames(ArticleTypes.df) <- MyRowNames
MyPCA <- prcomp(ArticleTypes.df)
PCA.df <- as.data.frame(matrix(nrow = nrow(MyPCA$x), ncol = 2))
colnames(PCA.df) <- colnames(MyPCA$x)[1:ncol(PCA.df)]
rownames(PCA.df) <- rownames(MyPCA$x)
PCA.df[,1] <- MyPCA$x[,1]
PCA.df[,2] <- MyPCA$x[,2]
library(ggplot2)
library(ggrepel)
##### Make plot of word frequency differences ####
WordDiffToNormalize <- WordDiffs.df$Freq_Diff
IndexGr8_1 <- which(WordDiffToNormalize > 1)
WordDiffToNormalize[IndexGr8_1] <- (WordDiffToNormalize[IndexGr8_1] / max(WordDiffToNormalize) + 1)
WordDiffs.df$Freq_Diff_Normalized <- WordDiffToNormalize

```

## Make Plots To Infer What It All Means

### How polar are the words in our vocabulary?

This first plot should show us the distribution of words in our vocabulary in terms of their polarity.

```{r}
p1 <- ggplot(data = WordDiffs.df, aes(Freq_Diff_Normalized))
#p1 <- p1 + geom_histogram (binwidth = 0.01)
p1 <- p1 + geom_density()
p1 <- p1 + labs (x = "Magnitude of polarity (ranges from 0 to 2, 1 being neutral", y = "Counts")
p1

```

This shows that most words are neutral, as we would expect. However, some words are indeed extremely polar.

Additionally, people are more creative in describing positive emotion, as they use more diverse set of words. However, people tend to be less creative in describing negative emotion Many words are occur relatively frequently in negative reviews.

### Can we classify media outlets by the sentiment they present?

Here we'll see if it is possible to extract patterns in media outlets's articles in an unsupervised manner.

```{r}
p2 <- ggplot (data = PCA.df, aes(PC1, PC2))
p2 <- p2 + geom_point()
p2 <- p2 + geom_text_repel(aes(PC1,PC2, label = rownames(PCA.df)))
p2 <- p2 + labs (title = "PCA plot of sentiment")
p2
```

These plots show quite clearly that the news media outlets differ quite drastically in the sentiment of their stories.

This preliminary analysis shows that it is possible to extract the differences in the sentiment conveyed by news articles. In fact, the sentiment conveyed is drastically different for the certain news media outlets.

Therefore, a more fine-grained and accurate classification using RNN (LSTM) is possible.
